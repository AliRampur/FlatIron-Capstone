{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Problem: Predicting Fraudulent Payment Transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitris Law has an online retail client who is losing over 10% of annual revenue to fraudulent payments via its online payment portal, which is significantly higher than the industry average (5%).\n",
    "\n",
    "They have asked my firm to develop a model that will identify potentially fraudulent payments and rank these transactions at the end of each weekly period. \n",
    "\n",
    "Using historical payment data extracted from their payment system, I can develop a classification prediction model to identify potential fraudulent transactions (i.e. payments) made by customers.\n",
    "\n",
    "Although this model is built on payment data for a specified location and contains fields that may not readily be available in other transactional datasets, a similar model can be built for various types of electronic/ACH, credit card, online, P2P transactions, etc.\n",
    "\n",
    "The source transactional data was extracted from Kaggle contributor: https://www.kaggle.com/turkayavci\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alira\\Anaconda3\\envs\\learn-env\\lib\\site-packages\\thefuzz\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "#import the necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xlrd\n",
    "import os\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, \\\n",
    "ExtraTreesClassifier, VotingClassifier, StackingRegressor, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.metrics import plot_confusion_matrix, recall_score,\\\n",
    "    accuracy_score, precision_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer,  make_column_selector as selector\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImPipeline\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "import pickle\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "from thefuzz import fuzz, process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. EDA: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in the modeling process is to perform exploratory data analysis. This includes:\n",
    "   - Understanding the total quantity of transactions available\n",
    "   - Identifying the variables that are available in the data, and whether they are binary, continuous or categorical\n",
    "   - Identifying whether any null values or NAs exist within the dataset that need to be removed or replaced\n",
    "   - Performing inferential analysis on certain variables. In this case, I will analyze the level of fraud by category, gender and age group\n",
    "   - Analyzing the target variable for potential class imbalance\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>customer</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>zipcodeOri</th>\n",
       "      <th>merchant</th>\n",
       "      <th>zipMerchant</th>\n",
       "      <th>category</th>\n",
       "      <th>amount</th>\n",
       "      <th>fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>'C1093826151'</td>\n",
       "      <td>'4'</td>\n",
       "      <td>'M'</td>\n",
       "      <td>'28007'</td>\n",
       "      <td>'M348934600'</td>\n",
       "      <td>'28007'</td>\n",
       "      <td>'es_transportation'</td>\n",
       "      <td>4.55</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>'C352968107'</td>\n",
       "      <td>'2'</td>\n",
       "      <td>'M'</td>\n",
       "      <td>'28007'</td>\n",
       "      <td>'M348934600'</td>\n",
       "      <td>'28007'</td>\n",
       "      <td>'es_transportation'</td>\n",
       "      <td>39.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>'C2054744914'</td>\n",
       "      <td>'4'</td>\n",
       "      <td>'F'</td>\n",
       "      <td>'28007'</td>\n",
       "      <td>'M1823072687'</td>\n",
       "      <td>'28007'</td>\n",
       "      <td>'es_transportation'</td>\n",
       "      <td>26.89</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>'C1760612790'</td>\n",
       "      <td>'3'</td>\n",
       "      <td>'M'</td>\n",
       "      <td>'28007'</td>\n",
       "      <td>'M348934600'</td>\n",
       "      <td>'28007'</td>\n",
       "      <td>'es_transportation'</td>\n",
       "      <td>17.25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>'C757503768'</td>\n",
       "      <td>'5'</td>\n",
       "      <td>'M'</td>\n",
       "      <td>'28007'</td>\n",
       "      <td>'M348934600'</td>\n",
       "      <td>'28007'</td>\n",
       "      <td>'es_transportation'</td>\n",
       "      <td>35.72</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   step       customer  age gender zipcodeOri       merchant zipMerchant  \\\n",
       "0     0  'C1093826151'  '4'    'M'    '28007'   'M348934600'     '28007'   \n",
       "1     0   'C352968107'  '2'    'M'    '28007'   'M348934600'     '28007'   \n",
       "2     0  'C2054744914'  '4'    'F'    '28007'  'M1823072687'     '28007'   \n",
       "3     0  'C1760612790'  '3'    'M'    '28007'   'M348934600'     '28007'   \n",
       "4     0   'C757503768'  '5'    'M'    '28007'   'M348934600'     '28007'   \n",
       "\n",
       "              category  amount  fraud  \n",
       "0  'es_transportation'    4.55      0  \n",
       "1  'es_transportation'   39.68      0  \n",
       "2  'es_transportation'   26.89      0  \n",
       "3  'es_transportation'   17.25      0  \n",
       "4  'es_transportation'   35.72      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bankdata_df_orig = pd.read_csv(\"./bs140513_032310_csv.csv\")\n",
    "bankdata_df_orig.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dictionary:\n",
    "\n",
    "_Step:_ This feature represents the day from the start of the data aggregation. It covers a period of 6 months.\n",
    "\n",
    "_Customer:_ Customer id\n",
    "\n",
    "_zipCodeOrigin:_ The zip code of origin/source.\n",
    "\n",
    "_Merchant:_ The merchant id\n",
    "\n",
    "_zipMerchant:_ The merchant zip code\n",
    "\n",
    "_Age:_ Categorized age\n",
    "0: <= 18,\n",
    "1: 19-25,\n",
    "2: 26-35,\n",
    "3: 36-45,\n",
    "4: 46:55,\n",
    "5: 56:65,\n",
    "6: > 65\n",
    "U: Unknown\n",
    "\n",
    "_Gender:_ \n",
    "E : Enterprise\n",
    "F: Female\n",
    "M: Male\n",
    "U: Unknown\n",
    "\n",
    "_Category:_ Category of the purchase\n",
    "\n",
    "_Amount:_ Amount of the purchase\n",
    "\n",
    "_Fraud:_ Target - fraudulent(1) or not(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to volume of transactions in the dataset and the limited amount of RAM on my personal device, I had to randomly sample 100,000 payment transactions from the original source data. The original dataset had 7,200 fraudulent transactions whereas the sampled dataset used for my model has 1,198. Both the original and sampled dataset have approximately 1.2% fraudulent tranactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_df = bankdata_df_orig.sample(n=100000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "step           7200\n",
       "customer       7200\n",
       "age            7200\n",
       "gender         7200\n",
       "zipcodeOri     7200\n",
       "merchant       7200\n",
       "zipMerchant    7200\n",
       "category       7200\n",
       "amount         7200\n",
       "fraud          7200\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bankdata_df_orig[bankdata_df_orig['fraud'] == 1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_df[bankdata_df['fraud'] == 1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_df['fraud'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_df['fraud'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that only 1.2% of the payments were identified as being fraudulent. This is a clear imbalanced dataset which I will have to address prior to building a predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_df.groupby('category')['amount'].sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_df[bankdata_df['fraud']== 1].groupby('category')['amount','fraud'].sum().sort_values(by='amount',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_df[bankdata_df['fraud']== 1].groupby('category')['amount'].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_df[bankdata_df['fraud']== 0].groupby('category')['amount'].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_df[bankdata_df['fraud']== 1]['amount'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the tables above, we can make the following observations on the dataset:\n",
    "- Total fraud across the xxxx transactions was xxxx\n",
    "- The most common (i.e. count) of fraud occurred in the 'Sports and Toys' category\n",
    "- The most costly (i.e. total dollars ) of fraud occurred in the travel category\n",
    "- The average fraudulent transaction was higher than the average non-fraudulent transaction in every category\n",
    "- There was no fraud identified in the transportation, food, and contents categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset does not have any null or any NA values, so no replacement or removal is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,15))\n",
    "sns.barplot(x=bankdata_df.category,y=bankdata_df.amount)\n",
    "plt.title(\"Bar Graph of Total Amount of Spend by Category\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,15))\n",
    "sns.barplot(x=bankdata_df.category,y=bankdata_df[bankdata_df['fraud']== 1].amount)\n",
    "plt.title(\"Bar Graph of Fraudulent Spend by Category\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,10))\n",
    "sns.scatterplot(x=bankdata_df.step,y=bankdata_df.amount,hue=bankdata_df.fraud)\n",
    "plt.title(\"Scatter Plot of Fraud Amount vs Day\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_df['zipcodeOri'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_df['zipMerchant'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is only one zipcode and 1 merchant zipcode, I will remove these from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_df = bankdata_df.drop(columns=['zipcodeOri','zipMerchant'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_df['customer'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I must preprocess the data to get it ready for the various modeling algorithms applicable for this type of classification problem. Preprocessing steps include:\n",
    "   - Creating the train and test datasets\n",
    "   - Applying SMOTE_NC to help alleviate the target class imbalance. SMOTE_NC can specifically be used on datasets that contain both continuous and categorical variables.\n",
    "   - Creating a pipeline that will apply standard scaler to continuous variables and one hot encode categorical variables, and then use column transformer\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankdata_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "subpipe_num = Pipeline(steps=[\n",
    "    ('ss', StandardScaler())\n",
    "])\n",
    "\n",
    "\n",
    "subpipe_cat = Pipeline(steps=[\n",
    "    ('ohe', OneHotEncoder(sparse=False, handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "CT = ColumnTransformer(transformers = [\n",
    "    ('subpipe_num', subpipe_num, selector(dtype_include = np.number)),\n",
    "    ('subpipe_cat', subpipe_cat, selector(dtype_include = object))\n",
    "], remainder = 'passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bankdata_df.drop(['fraud'],axis=1)\n",
    "y = bankdata_df['fraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    74126\n",
       "1      874\n",
       "Name: fraud, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to there being so few examples of fraud in the dataset (only 1.2%), I have applied SMOTENC below to increase the minority class of the train dataset to be at least 25% of the train majority class (i.e. not fraud)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_nc = SMOTENC(categorical_features=[1,2,3,4,5],sampling_strategy=.5,random_state=42)\n",
    "X_resampled, y_resampled = smote_nc.fit_resample(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    74126\n",
       "1    37063\n",
       "Name: fraud, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_resampled.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.666667\n",
       "1    0.333333\n",
       "Name: fraud, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_resampled.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the class is slightly more balanced, with 37,063 examples of the \"fraud\" target (or 33% of the sampled population)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modeling, Cross Validation and GridSearching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use the cross validation class taken from FlatIron lecture 51, and then apply certain types of classification models including:\n",
    "   - Dummy classifier\n",
    "   - Logistic Regression\n",
    "   - KNN\n",
    "   - Random Forest\n",
    "   - GradientBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWithCV():\n",
    "    '''Structure to save the model and more easily see its crossvalidation'''\n",
    "    def __init__(self, model, model_name, X, y, cv_now=True):\n",
    "        self.model = model\n",
    "        self.name = model_name\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        # For CV results\n",
    "        self.cv_results = None\n",
    "        self.cv_mean = None\n",
    "        self.cv_median = None\n",
    "        self.cv_std = None\n",
    "        #\n",
    "        if cv_now:\n",
    "            self.cross_validate()\n",
    "        \n",
    "    def cross_validate(self, X=None, y=None, kfolds=10):\n",
    "        '''\n",
    "        Perform cross-validation and return results.\n",
    "        \n",
    "        Args: \n",
    "          X:\n",
    "            Optional; Training data to perform CV on. Otherwise use X from object\n",
    "          y:\n",
    "            Optional; Training data to perform CV on. Otherwise use y from object\n",
    "          kfolds:\n",
    "            Optional; Number of folds for CV (default is 10)  \n",
    "        '''\n",
    "        \n",
    "        cv_X = X if X else self.X\n",
    "        cv_y = y if y else self.y\n",
    "\n",
    "        self.cv_results = cross_val_score(self.model, cv_X, cv_y, cv=kfolds)\n",
    "        self.cv_mean = np.mean(self.cv_results)\n",
    "        self.cv_median = np.median(self.cv_results)\n",
    "        self.cv_std = np.std(self.cv_results)\n",
    "\n",
    "        \n",
    "    def print_cv_summary(self):\n",
    "        cv_summary = (\n",
    "        f'''CV Results for `{self.name}` model:\n",
    "            {self.cv_mean:.5f} ± {self.cv_std:.5f} accuracy\n",
    "        ''')\n",
    "        print(cv_summary)\n",
    "    \n",
    "    def cvmean(self):\n",
    "        cvmean2 = round(self.cv_mean,3)\n",
    "        print(cvmean2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_model_pipe = Pipeline(steps= [\n",
    "    ('ct', CT),\n",
    "    ('dum', DummyClassifier(strategy = 'most_frequent'))\n",
    "])\n",
    "\n",
    "dummy_pipe = ModelWithCV(dummy_model_pipe, 'dummy model', X_resampled, y_resampled)\n",
    "dummy_pipe.print_cv_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_model_pipe = Pipeline([\n",
    "    ('ct', CT),\n",
    "    ('logreg', LogisticRegression(random_state=42, max_iter=5000))\n",
    "])\n",
    "\n",
    "logreg_pipe = ModelWithCV(logreg_model_pipe,'logreg_model', X_resampled, y_resampled)\n",
    "logreg_pipe.print_cv_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model_pipe = Pipeline([\n",
    "    ('ct',CT),\n",
    "    ('knn',KNeighborsClassifier(leaf_size=10000,n_neighbors=500))\n",
    "])\n",
    "\n",
    "knn_pipe = ModelWithCV(knn_model_pipe,'knn_model', X_resampled, y_resampled)\n",
    "knn_pipe.print_cv_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_model_pipe = Pipeline([\n",
    "    ('ct',CT),\n",
    "    ('rfc',RandomForestClassifier(random_state = 42,max_depth=9))\n",
    "])\n",
    "\n",
    "rfc_pipe = ModelWithCV(rfc_model_pipe,'rfc_model', X_resampled, y_resampled)\n",
    "rfc_pipe.print_cv_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc_model_pipe = Pipeline([\n",
    "    ('ct',CT),\n",
    "    ('gbc',GradientBoostingClassifier(random_state = 42))\n",
    "])\n",
    "\n",
    "gbc_pipe = ModelWithCV(gbc_model_pipe,'gbc_model', X_resampled, y_resampled)\n",
    "gbc_pipe.print_cv_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(logreg_model_pipe,open(\"logreg_fraud_model2.sav\",'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Gridsearch to RFC and KNN, which had very high accuracy scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuning and Cross Validating of RFC. Here we add in some of the important selection criteria for RFC.\n",
    "\n",
    "rfc_params = {}\n",
    "rfc_params['rfc__criterion'] = ['gini','entropy']\n",
    "rfc_params['rfc__min_samples_leaf'] = [5,10,15]\n",
    "rfc_params['rfc__max_depth'] = [5,7,9]\n",
    "\n",
    "\n",
    "knn_params = {}\n",
    "knn_params['knn__n_neighbors']=[300,500,700]\n",
    "knn_params['knn__weights']=['uniform','distance']\n",
    "knn_params['knn__algorithm']='auto',\n",
    "knn_params['knn__leaf_size']=[5000,10000]\n",
    "knn_params['knn__p']=[1,2]\n",
    "\n",
    "\n",
    "logreg_params = {}\n",
    "logreg_params['logreg__penalty']=['l2','none']\n",
    "#logreg_params['logreg__solver']= ['lbfgs', 'sag', 'saga']\n",
    "#'elasticnet', 'l1',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rfc_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_rfc = GridSearchCV(rfc_model_pipe, rfc_params, cv=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_rfc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_rfc.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(gs_rfc.cv_results_).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rfc_model_pipe = Pipeline([\n",
    "#     ('ct',CT),\n",
    "#     ('rfc',RandomForestClassifier(random_state = 42,max_depth=7))\n",
    "# ])\n",
    "\n",
    "# rfc_model_pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rfc_model_pipe.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rfc_model_pipe.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_knn = GridSearchCV(knn_model_pipe, knn_params, cv=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_knn.fit(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_knn.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_knn.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(gs_knn.cv_results_).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_logreg = GridSearchCV(logreg_model_pipe, logreg_params, cv=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_logreg.fit(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_logreg.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_logreg.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_logreg_model_pipe = Pipeline([\n",
    "    ('ct', CT),\n",
    "    ('logreg', LogisticRegression(random_state=42, max_iter=5000, penalty='none'))\n",
    "])\n",
    "\n",
    "Final_logreg_model_pipe.fit(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pickle.dump(Final_logreg_model_pipe,open(\"logreg_fraud_model.sav\",'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_logreg_model_pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = Final_logreg_model_pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot a confusion matric on actual and preditions of the test data.\n",
    "ax = plt.subplot()\n",
    "sns.set(font_scale=1) # Adjust to fit\n",
    "\n",
    "plot_confusion_matrix(rfc_model_pipe, X_test, y_test, ax=ax,  cmap = plt.cm.Greys, display_labels=['Not Fraud','Fraud'])\n",
    "\n",
    "# Labels, title and ticks\n",
    "label_font = {'size':'18'}  # Adjust to fit\n",
    "ax.set_xlabel('Predicted labels', fontdict=label_font);\n",
    "ax.set_ylabel('Observed labels', fontdict=label_font);\n",
    "ax.grid(b=False);\n",
    "\n",
    "title_font = {'size':'20'}  # Adjust to fit\n",
    "ax.set_title('Confusion Matrix', fontdict=title_font);\n",
    "\n",
    "plt.savefig('Confusion Matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = pd.read_csv(\"./sample_transactions.csv\")\n",
    "sample_df_2 = sample_df[['step','customer','age','gender','merchant','category','amount']]\n",
    "\n",
    "loaded_model_knn = pickle.load(open(\"knn_fraud_model.sav\", 'rb'))\n",
    "\n",
    "preds = loaded_model_knn.predict(sample_df_2)\n",
    "preds_proba = loaded_model_knn.predict_proba(sample_df_2)\n",
    "df_prediction = pd.DataFrame(zip(preds,preds_proba[:,1]*100),columns=['Fraud Prediction','Probability(%)'])\n",
    "df_prediction['Potentially Fraudulent?'] = np.where(df_prediction['Fraud Prediction'] == 1, 'Yes', 'No')\n",
    "df_combined = sample_df_2.join(df_prediction)\n",
    "df_combined.sort_values(by='Probability(%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
